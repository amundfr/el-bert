"""
This script relies on files generated by "Wiki Entity Linker" from
the AD chair at the University of Freiburg.
    python -m scripts.evaluate_mention_categories -h
"""

import json
from collections import defaultdict, Counter
from src.toolbox import get_docs, get_knowledge_base
import argparse
import configparser
from scipy.stats.stats import pearsonr

CAT_TO_CODE = {
        'Orgs': 'Q43229',
        'Locations': 'Q27096213',
        'Persons': 'Q18336849',
        'Events': 'Q1656682'
    }

config = configparser.ConfigParser()
config.read('config.ini')

default_jsonl = '''/models/trained/20211129_1004/saved_20211129_1705\
/predictions/conll_test_cs_oo_kb_predictions.jsonl'''
default_sitelink_file = '/ex_data/qid_to_x/qid_to_sitelink.tsv'

parser = argparse.ArgumentParser(
            formatter_class=argparse.RawDescriptionHelpFormatter,
            description=__doc__
        )
parser.add_argument(
        "--jsonl_file", type=str, default=default_jsonl,
        help=f"a .jsonl file generated with link_benchmark_entities.py"
             f" in project wiki_entity_linker. Default is: {default_jsonl}"
    )
parser.add_argument(
        "--sitelink_file", type=str, default=default_sitelink_file,
        help="a .tsv file with qid_to_sitelink"
    )

args = parser.parse_args()
jsonl_file = args.jsonl_file

kb_wd = get_knowledge_base(config, wiki='data', with_cg=False)

conll_split = [946, 216, 231]
conll_ents = defaultdict(list)
for i_conll, doc in enumerate(get_docs(
            '/ex_data/annotated_datasets/conll-wikidata-iob-annotations'
        )):
    if i_conll < conll_split[0]:
        key = 'train'
    elif i_conll < (conll_split[0] + conll_split[1]):
        key = 'val'
    else:
        key = 'test'
    for token in doc.tokens:
        ent = token.true_label
        if ent not in ['I', 'O', 'B']:
            conll_ents[key] += [ent.replace('_', ' ')]

preds = []
labels = []
for line in open(jsonl_file):
    vals = json.loads(line)
    preds += [vals['entity_mentions']]
    labels += [vals['labels']]

labels_orgs = defaultdict(list)
labels_locations = defaultdict(list)
labels_persons = defaultdict(list)
labels_events = defaultdict(list)
labels_other = defaultdict(list)

for i, doc_labels in enumerate(labels):
    for label in doc_labels:
        if not kb_wd.in_kb(label['entity_id']):
            labels_other[i] += [label]
            continue
        added = False
        if 'Q18336849' in label['type']:
            labels_persons[i] += [label]
            added = True
        if 'Q43229' in label['type']:
            labels_orgs[i] += [label]
            added = True
        if 'Q27096213' in label['type']:
            labels_locations[i] += [label]
            added = True
        if 'Q1656682' in label['type']:
            labels_events[i] += [label]
            added = True
        if not added:
            labels_other[i] += [label]

preds_orgs = defaultdict(list)
preds_locations = defaultdict(list)
preds_persons = defaultdict(list)
preds_events = defaultdict(list)
preds_other = defaultdict(list)

for i_doc, doc_preds in enumerate(preds):
    for pred in doc_preds:
        if not kb_wd.in_kb(pred['id']):
            preds_other[i_doc] += [pred]
            continue
        if i_doc in labels_orgs \
                and pred['span'] in [
                    la['span'] for la in labels_orgs[i_doc]
                ]:
            preds_orgs[i_doc] += [pred]
        if i_doc in labels_locations \
                and pred['span'] in [
                    la['span'] for la in labels_locations[i_doc]
                ]:
            preds_locations[i_doc] += [pred]
        if i_doc in labels_persons \
                and pred['span'] in [
                    la['span'] for la in labels_persons[i_doc]
                ]:
            preds_persons[i_doc] += [pred]
        if i_doc in labels_events \
                and pred['span'] in [
                    la['span'] for la in labels_events[i_doc]
                ]:
            preds_events[i_doc] += [pred]
        if i_doc in labels_other \
                and pred['span'] in [
                    la['span'] for la in labels_other[i_doc]
                ]:
            preds_other[i_doc] += [pred]


def mean_std(li):
    mean = sum(li) / len(li)
    var = sum((x - mean)**2 for x in li) / len(li)
    std_dev = var ** 0.5
    return mean, std_dev


def print_pred_label_info(
            category: str,
            cat_preds: list,
            cat_labels: list,
            qid_to_sitelink: dict = None,
        ):
    if len(qid_to_sitelink) > 0:
        include_score = True
    else:
        include_score = False

    def results_dict(
            label, label_in_train, pred, pred_in_train, len_cs, label_score, cs
            ):
        result = {
                'label': label,
                'label_in_train': label_in_train,
                'pred': pred,
                'pred_in_train': pred_in_train,
                'len_cs': len_cs,
                'label_score': label_score,
                'cs': cs,
            }
        return result

    # Sort predictions with correct MD in correct and wrong
    correct = []
    wrong = []
    only_current_cat = []
    for i_doc, doc_preds in cat_preds.items():
        for pred in doc_preds:
            for la in cat_labels[i_doc]:
                # if span is same
                if la['span'] == pred['span']:
                    # Check if this label is only in this category
                    # if la['type'] == CAT_TO_CODE[category]:
                    only_current_cat += [(pred, la)]
                    # Check if label is in train-data or not
                    if la['entity_id'] in conll_ents['train']:
                        label_in_train = True
                    else:
                        label_in_train = False
                    # Check if predicted entity is in train
                    if pred['id'] in conll_ents['train']:
                        pred_in_train = True
                    else:
                        pred_in_train = False
                    if include_score:
                        try:
                            score = qid_to_sitelink[la['entity_id']]
                        except KeyError:
                            score = 0
                    else:
                        score = 0
                    result = results_dict(
                                la['entity_id'], label_in_train,
                                pred['id'], pred_in_train,
                                len(pred['candidates']), score,
                                pred['candidates'],
                            )
                    # if label is same
                    if la['entity_id'] == pred['id']:
                        correct += [result]
                    # if not label is same
                    else:
                        wrong += [result]

    label_ids = [la['entity_id'] for dl in cat_labels.values() for la in dl]
    n_in_train = sum(1 for e in label_ids if e in conll_ents['train'])
    n_not_in_train = sum(1 for e in label_ids if e not in conll_ents['train'])

    n_predictions = sum(len(p) for p in cat_preds.values())
    correct_in_train = sum(1 for p in correct if p['label_in_train'])
    correct_not_in_train = sum(1 for p in correct if not p['label_in_train'])
    wrong_l_in_train = sum(1 for p in wrong if p['label_in_train'])
    wrong_l_not_in_train = sum(1 for p in wrong if not p['label_in_train'])
    wrong_p_in_train = sum(1 for p in wrong if p['pred_in_train'])
    wrong_p_not_in_train = sum(1 for p in wrong if not p['pred_in_train'])
    p_in_train = correct_in_train + wrong_l_in_train
    p_not_in_train = correct_not_in_train + wrong_l_not_in_train

    print(f"Category: {category}")

    print("  {} labels ({:.2f} % in Train ({} in {}, not in)".format(
        len(label_ids),
        100 * n_in_train/(n_in_train+n_not_in_train),
        n_in_train,
        n_not_in_train,
    ))

    if include_score:
        scores = [
                qid_to_sitelink[la['entity_id']]
                for dl in cat_labels.values()
                for la in dl if la['entity_id'] in qid_to_sitelink
            ]
        print("  Average label score: {:.2f} (n = {})".format(
            sum(scores) / len(scores), len(scores)
        ))

    print("  {} preds ({:.2f} % label in Train ({} in, {} not in)".format(
        n_predictions,
        100 * p_in_train/(p_in_train + p_not_in_train),
        p_in_train,
        p_not_in_train,
    ))
    predictions_counter = Counter([
            p['label'] for p in correct]
            + [p['label'] for p in wrong])
    print("  {} Unique entities".format(
        len(predictions_counter)
    ))
    print("  {:.2f} Avg. candidates".format(
        (sum(p['len_cs'] for p in correct) + sum(p['len_cs'] for p in wrong))
        / (len(correct) + len(wrong))
    ))
    if include_score:
        scores = \
            [p['label_score'] for p in wrong if not p['label_score'] == 0] + \
            [p['label_score'] for p in correct if not p['label_score'] == 0]
        print("  Average pred label score: {:.2f} (n = {})".format(
            sum(scores) / len(scores), len(scores)
        ))

    print("""\
  {:.2f} % precision
    {} correct predictions,
       {:.2f} % of labels are in train,
       {:.2f} candidates on average""".format(
        100*(len(correct)/(len(correct)+len(wrong))),
        len(correct),
        100 * (correct_in_train/(correct_in_train + correct_not_in_train)),
        sum(p['len_cs'] for p in correct)/len(correct),
    ))
    print("       {} unique entities correct".format(
            len(set(p['label'] for p in correct))
        ))
    if include_score:
        scores = \
            [p['label_score'] for p in correct if not p['label_score'] == 0]
        print("       {:.2f} average correct label score (n = {})".format(
            sum(scores) / len(scores), len(scores)
        ))

    print("""\
    {} wrong predictions,
       {:.2f} % of labels are in train, {:.2f} % of preds are in train,
       {:.2f} candidates on average""".format(
        len(wrong),
        100 * (wrong_l_in_train/(wrong_l_in_train + wrong_l_not_in_train)),
        100 * (wrong_p_in_train/(wrong_p_in_train + wrong_p_not_in_train)),
        sum(p['len_cs'] for p in wrong)/len(wrong),
    ))
    print("       {} unique entities wrong".format(
            len(set(p['label'] for p in wrong))
        ))
    if include_score:
        scores = \
            [p['label_score'] for p in wrong if not p['label_score'] == 0]
        print("       {:.2f} average wrong label score (n = {})".format(
            sum(scores) / len(scores), len(scores)
        ))

    print("""\
    {:.2f} % precision of preds in train (MD recall: {:.2f})
    {:.2f} % precision of preds not in train (MD recall: {:.2f})""".format(
        100*correct_in_train/p_in_train,
        100*(correct_in_train+wrong_l_in_train)/(n_in_train),
        100*correct_not_in_train/p_not_in_train,
        100*(correct_not_in_train+wrong_l_not_in_train)/(n_not_in_train),
    ))

    freq_train = Counter(conll_ents['train'])
    scores_correct = [
            (d['label'], freq_train[d['label']], d['label_score'])
            for d in correct
            if d['label_in_train']
        ]
    scores_wrong = [
            (d['label'], freq_train[d['label']], d['label_score'])
            for d in wrong
            if d['label_in_train']
        ]
    freq_correct = [e[1] for e in scores_correct]
    pop_correct = [e[2] for e in scores_correct]
    corr_correct, pr_correct = pearsonr(freq_correct, pop_correct)
    freq_wrong = [e[1] for e in scores_wrong]
    pop_wrong = [e[2] for e in scores_wrong]
    corr_wrong, pr_wrong = pearsonr(freq_wrong, pop_wrong)
    print(f"  For {len(scores_correct)} / {len(correct)} "
          f"CORRECT SEEN entities in KB")
    print("    Correlation of AIDA-CoNLL Train frequency and sitelink count")
    print(f"      correlation: {corr_correct:.4f}, p-value: {pr_correct:.4E}")
    print(f"  For {len(scores_wrong)} / {len(wrong)} WRONG SEEN entities inKB")
    print("    Correlation of AIDA-CoNLL Train frequency and sitelink count")
    print(f"      correlation: {corr_wrong:.4f}, p-value: {pr_wrong:.4E}")

    gt_pos_correct = [m['cs'].index(m['label']) for m in correct]
    avg_label_pos_correct = sum(gt_pos_correct)/len(gt_pos_correct)
    gt_pos_wrong = [
            m['cs'].index(m['label']) for m in wrong if m['label'] in m['cs']
        ]
    # Use as proxy for wether this is using CG
    if len(gt_pos_wrong) > 0:
        avg_label_pos_wrong = sum(gt_pos_wrong)/len(gt_pos_wrong)

        print("  For {} / {} Correct entities: ".format(
            len(gt_pos_correct), len(correct)), end='')
        print(
            "GT Label's avg position in Candidate Set is: {:.4f} (median: {})"
            .format(
                avg_label_pos_correct,
                gt_pos_correct[int(len(gt_pos_correct)/2)]
            ))
        print("  For {} / {} Wrong entities: ".format(
            len(gt_pos_wrong), len(wrong)), end='')
        print(
            "GT Label's avg position in Candidate Set is: {:.4f} (median: {})"
            .format(
                avg_label_pos_wrong,
                gt_pos_wrong[int(len(gt_pos_wrong)/2)]
            ))

    wrong_unseen_predicts_seen = [
            pred
            for pred in wrong
            if not pred['label_in_train']
            and pred['pred'] in conll_ents['train']
        ]
    wrong_unseen_predicts_unseen = [
            pred
            for pred in wrong
            if not pred['label_in_train']
            and not pred['pred'] in conll_ents['train']
        ]
    correct_unseen = [
            pred
            for pred in correct
            if not pred['label_in_train']
        ]
    wrong_seen_predicts_unseen = [
            pred
            for pred in wrong
            if pred['label_in_train']
            and not pred['pred'] in conll_ents['train']
        ]
    print(
        f"     {len(wrong_unseen_predicts_seen)} unseen wrong mentions "
        "where model predicts seen entity\n"
        f" {len(wrong_unseen_predicts_unseen)} unseen wrong mentions where"
        " model predicts unseen entity\n"
        f" {len(correct_unseen)} unseen correct mentions "
        "(i.e. model predicts unseen)\n"
        f" {len(wrong_seen_predicts_unseen)} seen wrong mentions where "
        "model predicts unseen\n"
    )
    print(" {}/{} = {:.2f} % of unseen labels, model predicts unseen entity"
          .format(
            len(correct_unseen)+len(wrong_unseen_predicts_unseen),
            len(correct_unseen)
            + len(wrong_unseen_predicts_unseen)
            + len(wrong_unseen_predicts_seen),
            100 * (len(correct_unseen)+len(wrong_unseen_predicts_unseen))
            / (len(correct_unseen)
               + len(wrong_unseen_predicts_unseen)
               + len(wrong_unseen_predicts_seen)
               )
          ))

    # Find candidate similarity of N candidates for mentions in each category
    min_cand = 2
    avg_candidate_sim = []
    for pred, label in only_current_cat:
        label_vec = kb_wd.get_entity_vector(label['entity_id'])
        if label_vec is None:
            continue
        cand_sim = kb_wd.candidate_similarity(label_vec, pred['candidates'])
        if len(cand_sim) >= min_cand:
            avg_sim_top = sum(sim[1] for sim in cand_sim[:min_cand]) / min_cand
            avg_candidate_sim += [avg_sim_top]
    # use as proxy for whether using CG or not
    if len(avg_candidate_sim) > 0:
        mean, std = mean_std(avg_candidate_sim)

        print("  {} / {} labels ({:.2f} %) have at least {} candidates"
              .format(
                len(avg_candidate_sim),
                len(only_current_cat),
                100*len(avg_candidate_sim)/len(only_current_cat),
                min_cand,
              ))
        print("    {:.2f} mean similarity (std {:.2f}) of top 5 candidates"
              .format(
                100*mean, 100*std,
              ))
        print("    {:.2f} min, {:.2f} max".format(
            100 * min(avg_candidate_sim),
            100 * max(avg_candidate_sim),
        ))

    print()


qid_to_sitelink = {}
if args.sitelink_file:
    for line in open(args.sitelink_file):
        vals = line.split()
        qid_to_sitelink[vals[0]] = int(vals[1])

# print_pred_label_info('Other (also OO-KB)', preds_other, labels_other)
print_pred_label_info(
        'Orgs', preds_orgs, labels_orgs, qid_to_sitelink
    )
print_pred_label_info(
        'Locations', preds_locations, labels_locations, qid_to_sitelink
    )
print_pred_label_info(
        'Persons', preds_persons, labels_persons, qid_to_sitelink
    )
print_pred_label_info(
        'Events', preds_events, labels_events, qid_to_sitelink
    )
print()


# for labels, type in zip(
#             [labels_orgs, labels_locations, labels_persons, labels_events],
#             ['Q43229', 'Q27096213', 'Q18336849', 'Q1656682']
#         ):
#     labl_cnt = Counter(
#                 la['entity_id'] for las in labels.values() for la in las
#                 if la['type'] == type
#             )

#     print("Most frequent labels: {}".format(
#         sorted(list(labl_cnt.items()), key=lambda x: x[1], reverse=True)[:10]
#     ))
